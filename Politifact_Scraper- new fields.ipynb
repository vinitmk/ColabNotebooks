{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Politifact_Scraper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dqiBejoCWqB"
      },
      "source": [
        "import urllib.request,sys,time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xePU28FxuaEf"
      },
      "source": [
        "def get_Score(url):\n",
        "  try:\n",
        "      page=requests.get(url)                             # this might throw an exception if something goes wrong\n",
        "  except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "      error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "      print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
        "      print (error_type, 'Line:', error_info.tb_lineno)\n",
        "  time.sleep(2)   \n",
        "  soup=BeautifulSoup(page.text,'html.parser')\n",
        "  #links=soup.find_all('article',attrs={'class':'o-platform__content'})\n",
        "  links=soup.find_all('div',attrs={'class':'m-scorecard__body'})\n",
        "  #print(len(links))\n",
        "  Score = []\n",
        "\n",
        "  for j in links:\n",
        "    TrueScore = j.find('p').find('a').text.strip()\n",
        "    Score.append(TrueScore)\n",
        "  return Score"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lYiLMLxx0j_"
      },
      "source": [
        "s = get_Score('https://www.politifact.com/personalities/facebook-posts/')\n",
        "print(s[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLrGARwtCdlE"
      },
      "source": [
        "pagesToGet= \n",
        "upperframe=[]  \n",
        "for page in range(1,pagesToGet+1):\n",
        "    print('processing page :', page)\n",
        "    url = 'https://www.politifact.com/factchecks/list/?page='+str(page)\n",
        "    print(url)\n",
        "\n",
        "    try:\n",
        "        #use the browser to get the url. This is suspicious command that might blow up.\n",
        "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
        "    \n",
        "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
        "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
        "        continue                                              #ignore this page. Abandon this and go back.\n",
        "    time.sleep(2)   \n",
        "    soup=BeautifulSoup(page.text,'html.parser')\n",
        "    frame=[]\n",
        "    links=soup.find_all('li',attrs={'class':'o-listicle__item'})\n",
        "    print(len(links))\n",
        "    filename=\"NEWS.csv\"\n",
        "    f=open(filename,\"w\", encoding = 'utf-8')\n",
        "    headers=\"label, statement, speaker, true, mostly_true, half_true, mostly_false, false, pants_on_fire, source, link ,date\\n\"\n",
        "    f.write(headers)\n",
        "\n",
        "    for j in links:\n",
        "      label = j.find('div', attrs ={'class':'m-statement__content'}).find('img',attrs={'class':'c-image__original'}).get('alt').strip()\n",
        "      statement = j.find(\"div\",attrs={'class':'m-statement__quote'}).text.strip()\n",
        "      speaker = j.find('div', attrs={'class':'m-statement__meta'}).find('a').text.strip()\n",
        "      Personality =  \"https://www.politifact.com\" + j.find('div',attrs={'class':'m-statement__meta'}).find('a')['href'].strip()\n",
        "      Score = get_Score(Personality)\n",
        "      true_score = Score[0]\n",
        "      mostly_true = Score[1]\n",
        "      half_true = Score[2]\n",
        "      mostly_false = Score[3]\n",
        "      false_score = Score[4]\n",
        "      pants_on_fire = Score[5]\n",
        "      source = j.find(\"div\",attrs={'class':'m-statement__desc'}).text.strip()\n",
        "      link = \"https://www.politifact.com\"\n",
        "      link += j.find(\"div\",attrs={'class':'m-statement__quote'}).find('a')['href'].strip()\n",
        "      date = j.find('div',attrs={'class':'m-statement__body'}).find('footer').text[-14:-1].strip()\n",
        "      frame.append((label, statement, speaker, true_score, mostly_true, half_true, mostly_false, false_score, pants_on_fire, source, link ,date))\n",
        "      #frame.append((Statement,Venue,Link,Date,Source,Label))\n",
        "      #f.write(Statement.replace(\",\",\"^\")+\",\"+Venue.replace(\",\",\"^\")+\",\"+Link+\",\"+Date.replace(\",\",\"^\")+\",\"+Source.replace(\",\",\"^\")+\",\"+Label.replace(\",\",\"^\")+\"\\n\")\n",
        "    upperframe.extend(frame)\n",
        "#f.close()\n",
        "data=pd.DataFrame(upperframe, columns=['label', 'statement', 'speaker', \n",
        "                                       'true_score', 'mostly_true', 'half_true', \n",
        "                                       'mostly_false', 'false_score', 'pants_on_fire', 'source', \n",
        "                                       'link' ,'date'])\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i01-dxpc_v0F",
        "outputId": "c591cb07-00b9-485f-ccd7-0f766a0bbda3"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpKl2RuTKGRG"
      },
      "source": [
        "data.to_csv(\"polifact-2.csv\")"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4cvFuvGktLb",
        "outputId": "3dc9f951-d421-4f54-a9fa-004b3eb22b3e"
      },
      "source": [
        "# from bs4 import BeautifulSoup\n",
        "# import pandas as pd\n",
        "# from tqdm import tqdm\n",
        "# from os import path\n",
        "# import requests\n",
        "# import hashlib\n",
        "# import time\n",
        "\n",
        "# TOTAL_PAGES = 573\n",
        "# col_index = ['name', 'quote_desc', 'quote', 'link', 'date_line', 'rating', 'sha256']\n",
        "# local_raw_data = \"./politifact_data.csv\"\n",
        "\n",
        "\n",
        "# def get_politifacts_page(page_num):\n",
        "#     page = requests.get(f\"https://www.politifact.com/factchecks/list/?page={page_num}\")\n",
        "#     soup = BeautifulSoup(page.content, 'html.parser')\n",
        "#     page_data = pd.DataFrame(columns=col_index)\n",
        "#     items = soup.findAll('article', class_=\"m-statement\")\n",
        "#     for item in items:\n",
        "#         # Who (or what) is the quote attributed to\n",
        "#         name = item.find('a', class_=\"m-statement__name\").get_text().strip()\n",
        "#         # Where was the quote made?\n",
        "#         quote_desc = item.find('div', class_=\"m-statement__desc\").get_text().strip()\n",
        "#         # What is the quote\n",
        "#         quote = item.find('div', class_=\"m-statement__quote\").find('a')\n",
        "#         link = f\"https://politifact.com{quote['href'].strip()}\"\n",
        "#         quote_text = quote.get_text().strip()\n",
        "#         # Date line with attribution, used for SHA-256 signature\n",
        "#         date_line = item.find('footer', class_=\"m-statement__footer\").get_text().strip()\n",
        "#         # Rating - Label to be predicted. Might filter some of these out later\n",
        "#         rating = item.find('div', class_=\"m-statement__meter\").find('picture').find('img')['alt']\n",
        "\n",
        "#         # SHA 256 hash used for identifying duplicate entries.\n",
        "#         # Will also be converted to an int for repeatable train/validation/test splits.\n",
        "#         sha256 = hashlib.sha256(f\"{name}{quote_desc}{quote_text}{link}{date_line}{rating}\".encode()).hexdigest()\n",
        "\n",
        "#         new_row = {col_index[0]: name, col_index[1]: quote_desc, col_index[2]: quote_text, col_index[3]: link,\n",
        "#                    col_index[4]: date_line, col_index[5]: rating, col_index[6]: sha256}\n",
        "#         page_data = page_data.append(new_row, ignore_index=True)\n",
        "#     return page_data\n",
        "\n",
        "\n",
        "# def update_politifact_data_set(end_page=1, start_page=1, data_set=None, wait_time=3):\n",
        "#     if data_set is None:\n",
        "#         data_set = pd.DataFrame(columns=col_index)\n",
        "\n",
        "#     if end_page > TOTAL_PAGES:\n",
        "#         end_page = TOTAL_PAGES\n",
        "\n",
        "#     for page_number in tqdm(range(end_page, start_page - 1, -1)):\n",
        "#         data_set = get_politifacts_page(page_num=page_number).append(data_set, ignore_index=True)\n",
        "#         # Delay between page requests. Don't be rude and slam their server.\n",
        "#         time.sleep(wait_time)\n",
        "\n",
        "#     # Remove any duplicate entries\n",
        "#     data_set = data_set.drop_duplicates(subset=\"sha256\")\n",
        "\n",
        "#     return data_set\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     if path.exists(local_raw_data):\n",
        "#         data = pd.read_csv(local_raw_data, sep='|')\n",
        "#     else:\n",
        "#         data = None\n",
        "\n",
        "#     data = update_politifact_data_set(5, data_set=data)\n",
        "\n",
        "#     data.to_csv(local_raw_data, header=True, index=False, sep='|')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:24<00:00,  4.83s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}